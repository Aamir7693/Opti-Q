{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4igAZs1pCiK"
      },
      "outputs": [],
      "source": [
        "%%writefile qoa.py\n",
        "#!/usr/bin/env python3\n",
        "import argparse\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s • %(levelname)s • %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
        "model.eval()\n",
        "\n",
        "def safe_str(obj: Any) -> str:\n",
        "    if isinstance(obj, (str, int, float)):\n",
        "        return str(obj)\n",
        "    if isinstance(obj, datetime):\n",
        "        return obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    if obj is None:\n",
        "        return \"\"\n",
        "    try:\n",
        "        if pd.isna(obj):\n",
        "            return \"\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    return str(obj)\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]  # (B, T, H)\n",
        "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    summed = torch.sum(token_embeddings * mask, dim=1)\n",
        "    denom = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
        "    return summed / denom  # (B, H)\n",
        "\n",
        "def chunk_text_words(text: str, chunk_size: int) -> List[str]:\n",
        "    words = (text or \"\").split()\n",
        "    if not words:\n",
        "        return [\"\"]\n",
        "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def embed_chunks_batched(chunks: List[str], batch_size: int) -> np.ndarray:\n",
        "    \"\"\"Return L2-normalized embeddings for each chunk, shape (N, D).\"\"\"\n",
        "    embs = []\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i:i+batch_size]\n",
        "        enc = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "        out = model(**enc)\n",
        "        pooled = mean_pooling(out, enc[\"attention_mask\"])\n",
        "        pooled = F.normalize(pooled, p=2, dim=1)\n",
        "        embs.append(pooled.detach().cpu().numpy())\n",
        "    return np.vstack(embs) if embs else np.zeros((0, model.config.hidden_size), dtype=np.float32)\n",
        "\n",
        "def build_text_embedding_map(\n",
        "    texts: List[str],\n",
        "    chunk_size_words: int = 250,\n",
        "    batch_size: int = 64\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Embed each unique text (with word-chunking) by:\n",
        "      1) splitting into chunks,\n",
        "      2) embedding all chunks in batches,\n",
        "      3) averaging chunk embeddings,\n",
        "      4) L2-normalizing final text embedding.\n",
        "    Returns {text: embedding_vector}.\n",
        "    \"\"\"\n",
        "    unique_texts = list(dict.fromkeys(texts))  # stable order, unique\n",
        "    # Build flat chunk list with reverse index to text\n",
        "    chunk_list: List[str] = []\n",
        "    chunk_owner: List[int] = []  # which text index this chunk belongs to\n",
        "\n",
        "    for ti, t in enumerate(unique_texts):\n",
        "        chunks = chunk_text_words(t, chunk_size_words)\n",
        "        for c in chunks:\n",
        "            chunk_list.append(c)\n",
        "            chunk_owner.append(ti)\n",
        "\n",
        "    logger.info(f\"Embedding {len(unique_texts)} unique texts via {len(chunk_list)} total chunks (batch_size={batch_size})\")\n",
        "    chunk_embs = embed_chunks_batched(chunk_list, batch_size=batch_size)  # (Nc, D)\n",
        "\n",
        "    D = chunk_embs.shape[1] if chunk_embs.size else model.config.hidden_size\n",
        "    sums = np.zeros((len(unique_texts), D), dtype=np.float32)\n",
        "    counts = np.zeros((len(unique_texts),), dtype=np.int32)\n",
        "\n",
        "    for ci, ti in enumerate(chunk_owner):\n",
        "        sums[ti] += chunk_embs[ci]\n",
        "        counts[ti] += 1\n",
        "\n",
        "    # average + normalize\n",
        "    text_map: Dict[str, np.ndarray] = {}\n",
        "    for ti, t in enumerate(unique_texts):\n",
        "        if counts[ti] == 0:\n",
        "            vec = np.zeros((D,), dtype=np.float32)\n",
        "        else:\n",
        "            vec = sums[ti] / float(counts[ti])\n",
        "            n = np.linalg.norm(vec)\n",
        "            if n > 0:\n",
        "                vec = vec / n\n",
        "        text_map[t] = vec\n",
        "\n",
        "    return text_map\n",
        "\n",
        "def compute_qoa_series(\n",
        "    annotated: pd.Series,\n",
        "    outputs: pd.Series,\n",
        "    text_emb: Dict[str, np.ndarray]\n",
        ") -> np.ndarray:\n",
        "    qoa = np.full((len(annotated),), np.nan, dtype=np.float32)\n",
        "\n",
        "    for i, (a_raw, o_raw) in enumerate(zip(annotated.values, outputs.values)):\n",
        "        a = safe_str(a_raw).strip()\n",
        "        o = safe_str(o_raw).strip()\n",
        "\n",
        "        if not a or not o:\n",
        "            qoa[i] = np.nan\n",
        "            continue\n",
        "\n",
        "        # Exact-match shortcut (case-insensitive containment)\n",
        "        if a.lower() in o.lower():\n",
        "            qoa[i] = 1.0\n",
        "            continue\n",
        "\n",
        "        ea = text_emb.get(a)\n",
        "        eo = text_emb.get(o)\n",
        "        if ea is None or eo is None:\n",
        "            qoa[i] = np.nan\n",
        "            continue\n",
        "\n",
        "        # Since vectors are L2-normalized, cosine == dot product\n",
        "        qoa[i] = float(np.dot(ea, eo))\n",
        "\n",
        "    return qoa\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser(description=\"Compute QoA (cosine similarity) for LLM outputs using sentence-transformers.\")\n",
        "    p.add_argument(\"-i\", \"--input-file\", required=True, help=\"Input CSV path\")\n",
        "    p.add_argument(\"-o\", \"--output-file\", required=True, help=\"Output CSV path\")\n",
        "    p.add_argument(\"--ann-col\", default=\"Annotated Answer\", help=\"Annotated answer column name\")\n",
        "    p.add_argument(\"--out-col\", default=\"Final_Output\", help=\"Model output column name\")\n",
        "    p.add_argument(\"--batch-size\", type=int, default=64, help=\"Embedding batch size\")\n",
        "    p.add_argument(\"--chunk-size-words\", type=int, default=250, help=\"Chunk size in words for long texts\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    logger.info(f\"Loading {args.input_file}\")\n",
        "    df = pd.read_csv(args.input_file, dtype=str)\n",
        "\n",
        "    if args.ann_col not in df.columns or args.out_col not in df.columns:\n",
        "        raise ValueError(f\"Missing required columns. Need: '{args.ann_col}' and '{args.out_col}'. Found: {list(df.columns)}\")\n",
        "\n",
        "    ann = df[args.ann_col].fillna(\"\").astype(str)\n",
        "    out = df[args.out_col].fillna(\"\").astype(str)\n",
        "\n",
        "    # Build embedding map for unique texts across both columns\n",
        "    all_texts = [safe_str(x).strip() for x in pd.concat([ann, out]).tolist()]\n",
        "    text_emb = build_text_embedding_map(\n",
        "        all_texts,\n",
        "        chunk_size_words=args.chunk_size_words,\n",
        "        batch_size=args.batch_size\n",
        "    )\n",
        "\n",
        "    logger.info(\"Computing QoA scores\")\n",
        "    df[\"Quality_of_Answer\"] = compute_qoa_series(ann, out, text_emb)\n",
        "\n",
        "    logger.info(f\"Saving results to {args.output_file}\")\n",
        "    df.to_csv(args.output_file, index=False)\n",
        "\n",
        "    logger.info(\"QoA summary:\\n\" + df[\"Quality_of_Answer\"].describe().to_string())\n",
        "    logger.info(\"Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}