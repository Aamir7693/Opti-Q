{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install networkx tiktoken tqdm\n"
      ],
      "metadata": {
        "id": "Zkw_d5P3qCMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update && sudo apt-get install -y zstd\n"
      ],
      "metadata": {
        "id": "drHYH7xVqD0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!pip -q install ollama networkx tiktoken tqdm pandas numpy\n"
      ],
      "metadata": {
        "id": "l3i2wrK-qDE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: try Vulkan GPU path (experimental)\n",
        "# If it breaks, remove OLLAMA_VULKAN=1 and restart the runtime.\n",
        "%env OLLAMA_HOST=127.0.0.1:11434\n",
        "%env OLLAMA_VULKAN=1\n",
        "\n",
        "!nohup ollama serve > ollama_server.log 2>&1 &\n",
        "!sleep 2\n",
        "!ollama --version\n",
        "!ollama list\n"
      ],
      "metadata": {
        "id": "HRlX-yu5qL8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3-chatqa:8b\n",
        "!ollama pull qwen2.5:14b\n",
        "!ollama pull mistral:7b\n",
        "!ollama pull phi4:14b\n",
        "!ollama pull gemma3:27b\n",
        "!ollama pull gemma2:27b\n",
        "!ollama list\n"
      ],
      "metadata": {
        "id": "x2MKZYdsqN_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ollama import Client\n",
        "import os\n",
        "OLLAMA_URL = os.environ.get(\"OLLAMA_URL\", \"http://127.0.0.1:11434\")\n",
        "client = Client(host=OLLAMA_URL)\n"
      ],
      "metadata": {
        "id": "HfGFItOLqPit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "id": "nUB0phUFqRCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4igAZs1pCiK"
      },
      "outputs": [],
      "source": [
        "%%writefile collection.py\n",
        "# paste your FULL script here exactly (no wrapping in code = r\"\"\" ... \"\"\")\n",
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import ollama  # type: ignore\n",
        "import tiktoken\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "import ast\n",
        "import ollama\n",
        "# Disable parallel tokenizers to avoid fork issues\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "\n",
        "# Logging and device setup\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s â€¢ %(levelname)s â€¢ %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Token counting\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(tokenizer.encode(text or \"\"))\n",
        "\n",
        "# LLM mapping\n",
        "llm_mapping = {\n",
        "    0: 'mistral:7b',\n",
        "    1: 'llama3-chatqa:8b',\n",
        "    2: 'phi4:14b',\n",
        "    3: 'qwen2.5:14b',\n",
        "    4: 'gemma3:27b',\n",
        "    5: 'blend',\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# ENHANCED QUERY CLEANING - REMOVES ALL COT ARTIFACTS\n",
        "# =============================================================================\n",
        "\n",
        "def clean_query(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove Chain-of-Thought prompting templates from queries.\n",
        "    Handles multiple formats and removes both prefix and suffix instructions.\n",
        "\n",
        "    Args:\n",
        "        text: Raw query text with CoT template\n",
        "\n",
        "    Returns:\n",
        "        Cleaned query text with only the essential question and options\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == \"\":\n",
        "        return text\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    # Step 1: Remove CoT template at beginning (if exists)\n",
        "    if \"Question:\" in text:\n",
        "        text = re.sub(r\"(?s)^.*?(?=Question:)\", \"\", text)\n",
        "\n",
        "    # Step 2: Remove trailing instructions and templates\n",
        "    patterns_to_remove = [\n",
        "        # Original pattern from your code\n",
        "        r\"(?s)Query:[\\s\\S]*?Only write the final answer without any additional explanation\\.\\s*\",\n",
        "\n",
        "        # Common trailing instruction\n",
        "        r\"\\n\\nAbove are multiple-choice questions.*?Simply respond with.*?\\.\",\n",
        "\n",
        "        # Other instruction patterns\n",
        "        r\"\\n\\nInstruction:.*?(?=\\n[A-Z]\\)|$)\",\n",
        "        r\"\\n\\nOnly write the final answer.*?$\",\n",
        "        r\"\\n\\nSimply respond with.*?$\",\n",
        "\n",
        "        # CoT reasoning templates\n",
        "        r\"\\n\\nIdentify the core query:.*?$\",\n",
        "        r\"\\n\\nRecall known facts:.*?$\",\n",
        "        r\"\\n\\nEliminate incorrect answers:.*?$\",\n",
        "        r\"\\n\\nConfirm the best answer:.*?$\",\n",
        "        r\"\\n\\nConclude with the final answer:.*?$\",\n",
        "\n",
        "        # Additional common patterns\n",
        "        r\"\\n\\nThink step by step.*?$\",\n",
        "        r\"\\n\\nProvide your reasoning.*?$\",\n",
        "        r\"\\n\\nExplain your answer.*?$\",\n",
        "        r\"\\n\\nShow your work.*?$\"\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns_to_remove:\n",
        "        text = re.sub(pattern, \"\", text, flags=re.DOTALL)\n",
        "\n",
        "    # Step 3: Clean up \"Question:\" prefix if it exists\n",
        "    if text.strip().startswith(\"Question:\"):\n",
        "        text = text.strip()[9:].strip()\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# =============================================================================\n",
        "# ULTRA-STRICT PROMPTS - ADDRESSING ALL IDENTIFIED GAPS\n",
        "# =============================================================================\n",
        "\n",
        "class UltraStrictPrompts:\n",
        "    \"\"\"Ultra-strict prompts designed to eliminate all failure patterns identified in data analysis\"\"\"\n",
        "\n",
        "    # BLEND PROMPTS - CRITICAL FOR SIMILARITY IMPROVEMENT\n",
        "    BLEND_MMLU = \"\"\"Question: {question}\n",
        "\n",
        "Candidate responses:\n",
        "{candidates}\n",
        "You are given several answers from different models.\n",
        "Combine them into a single, clear, and accurate response,\n",
        "resolving any redundancy or inconsistency.\n",
        "Task: Select the correct answer from candidates.\n",
        "\n",
        "Rules:\n",
        "- Choose the factually accurate option\n",
        "- If candidates disagree, select the correct one\n",
        "- If multiple are correct, pick any one\n",
        "- Use EXACT format below\n",
        "\n",
        "Required format: \"the answer is X)Option text.\"\n",
        "Example: \"the answer is B)Diabetes.\"\n",
        "\n",
        "Output (nothing else):\"\"\"\n",
        "\n",
        "    BLEND_SIMPLEQA = \"\"\"Question: {question}\n",
        "\n",
        "Candidate responses:\n",
        "{candidates}\n",
        "You are given several answers from different models.\n",
        "Combine them into a single, clear, and accurate response,\n",
        "resolving any redundancy or inconsistency.\n",
        "Task: Provide the most accurate answer.\n",
        "\n",
        "Rules:\n",
        "- Select the factually correct response\n",
        "- If responses disagree, choose the accurate one\n",
        "- Be direct and concise\n",
        "\n",
        "Output (nothing else):\"\"\"\n",
        "\n",
        "    # VERIFICATION PROMPTS - STRONG FORMAT CHECKING\n",
        "    VERIFICATION_MMLU = \"\"\"Question: {question}\n",
        "You are given an answer from another model.\n",
        "Use it as a supporting context to verify or improve your answer if\n",
        "it appears correct, or disregard it if it appears incorrect when\n",
        "answering the same question.\n",
        "Answer to verify: {context}\n",
        "\n",
        "Task: Check accuracy and format.\n",
        "\n",
        "Rules:\n",
        "- If correct and properly formatted, output it exactly\n",
        "- If wrong answer, provide correct answer\n",
        "- The \"Option text\" MUST be copied EXACTLY from the question's options (do not paraphrase).\n",
        "- Use format: \"the answer is X)Option text.\"\n",
        "- Output nothing else.\n",
        "\n",
        "Output (nothing else):\"\"\"\n",
        "\n",
        "    VERIFICATION_SIMPLEQA = \"\"\"Question: {question}\n",
        "\n",
        "Answer to verify: {context}\n",
        "\n",
        "Task: Output the correct final answer ONLY.\n",
        "\n",
        "Rules:\n",
        "- If the answer to verify is correct, OUTPUT IT EXACTLY (copy/paste).\n",
        "- If it is wrong, output the corrected answer ONLY.\n",
        "- Do NOT output: \"Correct\", \"Incorrect\", \"Yes\", \"No\", or any explanation.\n",
        "- Output must be a single line containing only the answer text.\n",
        "\n",
        "Examples:\n",
        "Answer to verify: 1998  -> Output: 1998\n",
        "Answer to verify: Iron -> Output: Gold\n",
        "\n",
        "Output (nothing else):\"\"\"\n",
        "\n",
        "\n",
        "    # LEAF PROMPTS - SIMPLE AND DIRECT\n",
        "    LEAF_MMLU = \"\"\"Question: {question}\n",
        "Above are multiple-choice questions (with answers) related to Science & Technology, Politics, Art, Geography,\n",
        "Sports, Music, TV Shows, History, Video Games, and other.\n",
        "Simply respond with:\n",
        "The answer is /option letter and value/.\n",
        "\n",
        "Output (nothing else):\"\"\"\n",
        "\n",
        "    LEAF_SIMPLEQA = \"\"\"Question: {question}\n",
        "Your job is to answer a question with a short and concise answer.\n",
        "The question will fall into one of the following categories: Science & Technology, Politics, Art, Geography,\n",
        "Sports, Music, TV Shows, History, Video Games, and other.\n",
        "Answer the following question directly with the correct, concise answer.\n",
        "\n",
        "\n",
        "Output (nothing else):\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# ULTRA-STRICT POST-PROCESSING - AGGRESSIVE FORMAT ENFORCEMENT\n",
        "# =============================================================================\n",
        "\n",
        "def ultra_strict_post_process(output: str, question_type: str) -> str:\n",
        "    \"\"\"Aggressive post-processing to enforce exact format matching annotated answers\"\"\"\n",
        "\n",
        "    if not output:\n",
        "        return \"\"\n",
        "\n",
        "    output = output.strip()\n",
        "\n",
        "    if question_type == 'mmlu':\n",
        "        # Force lowercase \"the answer is\" (CRITICAL for similarity)\n",
        "        if output.startswith(\"The answer is\"):\n",
        "            output = \"the\" + output[3:]\n",
        "\n",
        "        # Extract just the answer part, remove everything after first sentence\n",
        "        pattern = r'(the answer is [A-E]\\)[^.]*\\.?)'\n",
        "        match = re.search(pattern, output, re.IGNORECASE)\n",
        "        if match:\n",
        "            answer = match.group(1)\n",
        "            # Ensure it ends with period\n",
        "            if not answer.endswith('.'):\n",
        "                answer += '.'\n",
        "            # Force lowercase start\n",
        "            if answer.startswith('The'):\n",
        "                answer = 'the' + answer[3:]\n",
        "            return answer\n",
        "\n",
        "        # Fallback: try to extract option and format correctly\n",
        "        option_pattern = r'([A-E]\\)[^.]*)'\n",
        "        option_match = re.search(option_pattern, output)\n",
        "        if option_match:\n",
        "            option_text = option_match.group(1)\n",
        "            if not option_text.endswith('.'):\n",
        "                option_text += '.'\n",
        "            return f\"the answer is {option_text}\"\n",
        "\n",
        "    else:  # SimpleQA\n",
        "        # Remove explanation patterns aggressively\n",
        "        explanation_cuts = [\n",
        "            r'\\n\\nExplanation:.*$',\n",
        "            r'\\. This .*$',\n",
        "            r'\\. The reasoning.*$',\n",
        "            r'\\. Given .*$',\n",
        "            r'\\. Both .*$',\n",
        "            r'\\. However.*$',\n",
        "            r'\\. Therefore.*$',\n",
        "            r'\\. Based on.*$'\n",
        "        ]\n",
        "\n",
        "        for pattern in explanation_cuts:\n",
        "            output = re.sub(pattern, '', output, flags=re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "        # Take only first sentence for SimpleQA if it's too long\n",
        "        if len(output) > 100:\n",
        "            sentences = output.split('.')\n",
        "            if sentences and sentences[0].strip():\n",
        "                return sentences[0].strip() + '.'\n",
        "\n",
        "    return output\n",
        "\n",
        "# =============================================================================\n",
        "# ENHANCED DAG EXECUTOR WITH RETRY MECHANISM\n",
        "# =============================================================================\n",
        "\n",
        "class UltraStrictDAGExecutor:\n",
        "    \"\"\"DAG executor with extreme format enforcement and retry mechanism\"\"\"\n",
        "    def __init__(self, edges: List[Tuple[int,int]], node_models: Dict[int,str], fuser_model: str = \"gemma3:27b\"):\n",
        "        self.G = nx.DiGraph()\n",
        "        self.G.add_edges_from(edges)\n",
        "        self.G.add_nodes_from(node_models.keys())\n",
        "        self.fuser_model = fuser_model\n",
        "        for n, m in node_models.items():\n",
        "            self.G.nodes[n]['model'] = m\n",
        "\n",
        "    def determine_question_type(self, question: str, topic: str = \"\") -> str:\n",
        "        \"\"\"Determine if question is MMLU-style or SimpleQA-style\"\"\"\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Check for explicit MMLU indicators\n",
        "        if topic.endswith('_mmlu'):\n",
        "            return 'mmlu'\n",
        "\n",
        "        # Check for multiple choice indicators\n",
        "        if any(indicator in question for indicator in ['A)', 'B)', 'C)', 'D)', 'E)']):\n",
        "            return 'mmlu'\n",
        "\n",
        "        # Check for typical multiple choice patterns\n",
        "        if 'which of the following' in question_lower or 'select the' in question_lower:\n",
        "            return 'mmlu'\n",
        "\n",
        "        return 'simpleqa'\n",
        "\n",
        "    def format_candidates_minimal(self, candidates: List[str]) -> str:\n",
        "        \"\"\"Format candidate responses minimally\"\"\"\n",
        "        if not candidates:\n",
        "            return \"No candidates available.\"\n",
        "\n",
        "        formatted = []\n",
        "        for i, candidate in enumerate(candidates, 1):\n",
        "            clean_candidate = candidate.strip()\n",
        "            if not clean_candidate:\n",
        "                clean_candidate = \"[Empty response]\"\n",
        "            formatted.append(f\"{i}. {clean_candidate}\")\n",
        "\n",
        "        return \"\\n\".join(formatted)\n",
        "\n",
        "    def execute_with_validation(self, prompt: str, model: str, question_type: str, max_retries: int = 2):\n",
        "        \"\"\"Execute with format validation and retries if needed\"\"\"\n",
        "\n",
        "        for attempt in range(max_retries + 1):\n",
        "            resp = ollama.generate(model=model, prompt=prompt, options={\"device\": device})\n",
        "            raw_output = resp.get('response', '').strip()\n",
        "\n",
        "            # Apply ultra-strict post-processing\n",
        "            processed_output = ultra_strict_post_process(raw_output, question_type)\n",
        "\n",
        "            # Validate format for MMLU\n",
        "            if question_type == 'mmlu':\n",
        "                if processed_output.startswith('the answer is') and ')' in processed_output and processed_output.endswith('.'):\n",
        "                    return processed_output, raw_output\n",
        "                elif attempt < max_retries:\n",
        "                    # Retry with even more explicit format enforcement\n",
        "                    prompt += f\"\\n\\nCRITICAL: Must be exactly 'the answer is X)Option.' format. Example: 'the answer is A)Diabetes.'\"\n",
        "                    continue\n",
        "            else:\n",
        "                # For SimpleQA, accept if it's reasonably concise\n",
        "                if len(processed_output) < 200 and not any(word in processed_output.lower() for word in ['explanation', 'reasoning', 'analysis', 'therefore', 'however']):\n",
        "                    return processed_output, raw_output\n",
        "\n",
        "            # If we reach max retries, return what we have\n",
        "            if attempt == max_retries:\n",
        "                return processed_output, raw_output\n",
        "\n",
        "        return processed_output, raw_output\n",
        "\n",
        "    def build_prompt(self, node_type: str, question: str, candidates: List[str] = None, context: str = None, topic: str = \"\") -> str:\n",
        "        \"\"\"Build appropriate prompt based on node type\"\"\"\n",
        "        question_type = self.determine_question_type(question, topic)\n",
        "        clean_question = clean_query(question)\n",
        "\n",
        "        if node_type == 'blend':\n",
        "            candidates_block = self.format_candidates_minimal(candidates or [])\n",
        "            if question_type == 'mmlu':\n",
        "                return UltraStrictPrompts.BLEND_MMLU.format(\n",
        "                    question=clean_question,\n",
        "                    candidates=candidates_block\n",
        "                )\n",
        "            else:\n",
        "                return UltraStrictPrompts.BLEND_SIMPLEQA.format(\n",
        "                    question=clean_question,\n",
        "                    candidates=candidates_block\n",
        "                )\n",
        "\n",
        "        elif node_type == 'verification':\n",
        "            if question_type == 'mmlu':\n",
        "                return UltraStrictPrompts.VERIFICATION_MMLU.format(\n",
        "                    question=clean_question,\n",
        "                    context=context or \"\"\n",
        "                )\n",
        "            else:\n",
        "                return UltraStrictPrompts.VERIFICATION_SIMPLEQA.format(\n",
        "                    question=clean_question,\n",
        "                    context=context or \"\"\n",
        "                )\n",
        "\n",
        "        elif node_type == 'leaf':\n",
        "            if question_type == 'mmlu':\n",
        "                return UltraStrictPrompts.LEAF_MMLU.format(question=clean_question)\n",
        "            else:\n",
        "                return UltraStrictPrompts.LEAF_SIMPLEQA.format(question=clean_question)\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def execute(self, query: str, topic: str = \"\") -> Tuple[Dict[int,str], List[Dict[str,Any]]]:\n",
        "        \"\"\"Execute the DAG with ultra-strict prompting and validation\"\"\"\n",
        "        outputs: Dict[int, str] = {}\n",
        "        logs: List[Dict[str,Any]] = []\n",
        "        question_type = self.determine_question_type(query, topic)\n",
        "\n",
        "        for node in nx.topological_sort(self.G):\n",
        "            parents = list(self.G.predecessors(node))\n",
        "            model = self.G.nodes[node]['model']\n",
        "\n",
        "            # BLENDING NODES (Multiple parents or explicit 'blend')\n",
        "            if len(parents) > 1 or model.lower() == 'blend':\n",
        "                candidate_texts = [outputs[p] for p in parents]\n",
        "                prompt = self.build_prompt('blend', query, candidates=candidate_texts, topic=topic)\n",
        "\n",
        "                in_toks = count_tokens(prompt)\n",
        "                logger.info(f\"Blend Node {node}: fuser_model={self.fuser_model}, input_tokens={in_toks}\")\n",
        "\n",
        "                processed_output, raw_output = self.execute_with_validation(\n",
        "                    prompt, self.fuser_model, question_type\n",
        "                )\n",
        "                out_toks = count_tokens(processed_output)\n",
        "\n",
        "                outputs[node] = processed_output\n",
        "                logs.append({\n",
        "                    'node': node,\n",
        "                    'type': 'blend',\n",
        "                    'model': self.fuser_model,\n",
        "                    'prompt': prompt,\n",
        "                    'candidates': candidate_texts,\n",
        "                    'raw_output': raw_output,\n",
        "                    'output': processed_output,\n",
        "                    'input_tokens': in_toks,\n",
        "                    'output_tokens': out_toks\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # VERIFICATION NODES (Single parent)\n",
        "            elif len(parents) == 1:\n",
        "                context = outputs[parents[0]]\n",
        "                prompt = self.build_prompt('verification', query, context=context, topic=topic)\n",
        "\n",
        "                in_toks = count_tokens(prompt)\n",
        "                logger.info(f\"Verification Node {node}: model={model}, input_tokens={in_toks}\")\n",
        "\n",
        "                processed_output, raw_output = self.execute_with_validation(\n",
        "                    prompt, model, question_type\n",
        "                )\n",
        "                out_toks = count_tokens(processed_output)\n",
        "\n",
        "                outputs[node] = processed_output\n",
        "                logs.append({\n",
        "                    'node': node,\n",
        "                    'type': 'verification',\n",
        "                    'model': model,\n",
        "                    'prompt': prompt,\n",
        "                    'parent_output': context,\n",
        "                    'raw_output': raw_output,\n",
        "                    'output': processed_output,\n",
        "                    'input_tokens': in_toks,\n",
        "                    'output_tokens': out_toks\n",
        "                })\n",
        "\n",
        "            # LEAF NODES (No parents)\n",
        "            else:\n",
        "                prompt = self.build_prompt('leaf', query, topic=topic)\n",
        "\n",
        "                in_toks = count_tokens(prompt)\n",
        "                logger.info(f\"Leaf Node {node}: model={model}, input_tokens={in_toks}\")\n",
        "\n",
        "                processed_output, raw_output = self.execute_with_validation(\n",
        "                    prompt, model, question_type\n",
        "                )\n",
        "                out_toks = count_tokens(processed_output)\n",
        "\n",
        "                outputs[node] = processed_output\n",
        "                logs.append({\n",
        "                    'node': node,\n",
        "                    'type': 'leaf',\n",
        "                    'model': model,\n",
        "                    'prompt': prompt,\n",
        "                    'raw_output': raw_output,\n",
        "                    'output': processed_output,\n",
        "                    'input_tokens': in_toks,\n",
        "                    'output_tokens': out_toks\n",
        "                })\n",
        "\n",
        "        # Return final sink outputs\n",
        "        sinks = [n for n in self.G.nodes if self.G.out_degree(n) == 0]\n",
        "        final = {s: outputs[s] for s in sinks}\n",
        "        return final, logs\n",
        "\n",
        "# =============================================================================\n",
        "# UTILITY FUNCTIONS (Keep existing ones)\n",
        "# =============================================================================\n",
        "\n",
        "def get_edge_list(mask: int, k: int, llm_assignment: List[int]) -> List[Any]:\n",
        "    if k == 1:\n",
        "        model = llm_mapping[int(llm_assignment[0])]\n",
        "        return [([0], [model])]\n",
        "    B = [(mask >> bit) & 1 for bit in range(k * (k - 1) // 2)]\n",
        "    adj = np.zeros((k, k), dtype=int)\n",
        "    idx_b = 0\n",
        "    for i in range(k - 1):\n",
        "        for j in range(i + 1, k):\n",
        "            adj[i, j] = B[idx_b]; idx_b += 1\n",
        "    edge_list: List[Tuple[List[int], List[str]]] = []\n",
        "    for i in range(k):\n",
        "        for j in range(k):\n",
        "            if adj[i, j] == 1:\n",
        "                model_i = llm_mapping[int(llm_assignment[i])]\n",
        "                model_j = llm_mapping[int(llm_assignment[j])]\n",
        "                edge_list.append(([i, j], [model_i, model_j]))\n",
        "    return edge_list\n",
        "\n",
        "def build_plans_from_csv(df:pd.DataFrame) -> pd.DataFrame:\n",
        "    # df = pd.read_csv(csv_path)\n",
        "\n",
        "    # ENHANCED: Clean queries before processing\n",
        "    if 'Original_Query' in df.columns:\n",
        "        df['Original_Query'] = df['Original_Query'].apply(clean_query)\n",
        "\n",
        "    df['plan'] = None\n",
        "    for idx, row in df.iterrows():\n",
        "        llm_assignment_list = [x.strip() for x in str(row['assignment']).strip('()').split(',') if x.strip()]\n",
        "        mask = int(str(row['struct_id']).strip('()').split(',')[0])\n",
        "        plan = get_edge_list(mask=mask, k=len(llm_assignment_list), llm_assignment=llm_assignment_list)\n",
        "        df.at[idx, 'plan'] = str(plan)\n",
        "    return df\n",
        "\n",
        "# =============================================================================\n",
        "# ULTRA-STRICT RUNNER FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def run_plan_queries_ultra_strict(\n",
        "    df: pd.DataFrame,\n",
        "    plan_col: str = 'plan',\n",
        "    query_col: str = 'Original_Query',\n",
        "    assignment_col: str = 'assignment',\n",
        "    topic_col: str = 'query_type'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Ultra-strict plan runner focused on exact format matching\"\"\"\n",
        "    results: List[Dict[str,Any]] = []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing with ultra-strict prompts\"):\n",
        "        plan_raw = row[plan_col]\n",
        "        plan = ast.literal_eval(plan_raw) if isinstance(plan_raw, str) else plan_raw\n",
        "        topic = str(row.get(topic_col, \"\")).strip()\n",
        "\n",
        "        # Handle plan structure\n",
        "        if isinstance(plan, list) and plan and all(isinstance(x, int) for x in plan):\n",
        "            # Single-node plan\n",
        "            model_str = str(row[assignment_col]).strip('()').split(',')[0].strip()\n",
        "            model = llm_mapping[int(model_str)] if model_str.isdigit() else model_str\n",
        "            node_models = {i: model for i in plan}\n",
        "            edges = []\n",
        "        else:\n",
        "            # Multi-node plan\n",
        "            edges = []\n",
        "            node_models = {}\n",
        "            for entry in plan:\n",
        "                if isinstance(entry[0], list) and len(entry[0]) == 1:\n",
        "                    # Single-node plan like ([0], ['gemma:2b'])\n",
        "                    i = entry[0][0]\n",
        "                    model_i = entry[1][0]\n",
        "                    node_models[i] = model_i\n",
        "                else:\n",
        "                    (i, j), (m_i, m_j) = entry\n",
        "                    node_models[i] = m_i\n",
        "                    node_models[j] = m_j\n",
        "                    edges.append((i, j))\n",
        "\n",
        "        # Execute with ultra-strict DAG executor\n",
        "        executor = UltraStrictDAGExecutor(edges, node_models, fuser_model=\"gemma3:27b\")\n",
        "        final_outs, logs = executor.execute(row[query_col], topic)\n",
        "\n",
        "        # Prepare result\n",
        "        out = row.to_dict()\n",
        "        out.update({\n",
        "            'Final_Output': ' '.join(final_outs.values()),\n",
        "            'step_logs': json.dumps(logs, indent=2),\n",
        "            'num_nodes': len(node_models),\n",
        "            'num_edges': len(edges),\n",
        "            'question_type': executor.determine_question_type(row[query_col], topic)\n",
        "        })\n",
        "        results.append(out)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def run_with_checkpoint(\n",
        "    df: pd.DataFrame,\n",
        "    checkpoint_every: int,\n",
        "    output_path: str\n",
        ") -> pd.DataFrame:\n",
        "    results: List[Dict[str,Any]] = []\n",
        "    total = len(df)\n",
        "    logger.info(f\"Starting collection of {total} rows; checkpoint every {checkpoint_every}\")\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=total, desc=\"Collecting\"):\n",
        "        plan = ast.literal_eval(row['plan'])\n",
        "        # build edges & node_models as in your existing code\n",
        "        edges, node_models = [], {}\n",
        "        for pair, models in plan:\n",
        "            if len(pair) == 1:\n",
        "                node_models[pair[0]] = models[0]\n",
        "            else:\n",
        "                i, j = pair\n",
        "                node_models[i], node_models[j] = models[0], models[1]\n",
        "                edges.append((i, j))\n",
        "\n",
        "        exec = UltraStrictDAGExecutor(edges, node_models, fuser_model=\"gemma3:27b\")\n",
        "        final_outs, logs = exec.execute(row['Original_Query'], row.get('query_type', ''))\n",
        "        out = row.to_dict()\n",
        "        out.update({\n",
        "            'Final_Output': ' '.join(final_outs.values()),\n",
        "            'step_logs': json.dumps(logs)\n",
        "        })\n",
        "        results.append(out)\n",
        "\n",
        "        # checkpoint\n",
        "        if checkpoint_every and (idx + 1) % checkpoint_every == 0:\n",
        "            chk = f\"{os.path.splitext(output_path)[0]}.checkpoint.csv\"\n",
        "            pd.DataFrame(results).to_csv(chk, index=False)\n",
        "            logger.info(f\"[checkpoint @ {idx+1}] wrote {len(results)} rows → {chk}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# =============================================================================\n",
        "# EXAMPLE USAGE\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser(description=\"UltraStrict collection with checkpointing\")\n",
        "    p.add_argument(\"-i\", \"--input-csv\",  required=True, help=\"Input CSV path\")\n",
        "    p.add_argument(\"-o\", \"--output-csv\", required=True, help=\"Output CSV path\")\n",
        "    p.add_argument(\"--checkpoint-every\", type=int, default=100,\n",
        "                   help=\"Checkpoint every N rows\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    logger.info(f\"Loading input: {args.input_csv}\")\n",
        "    df = pd.read_csv(args.input_csv)\n",
        "    df = build_plans_from_csv(df)\n",
        "\n",
        "    df_out = run_with_checkpoint(\n",
        "        df,\n",
        "        checkpoint_every=args.checkpoint_every,\n",
        "        output_path=args.output_csv\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Writing final output to {args.output_csv}\")\n",
        "    df_out.to_csv(args.output_csv, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}